
@article{holt_object-based_2009,
	title = {Object-based detection and classification of {Vehicles} from high-resolution aerial photography},
	volume = {75},
	issn = {0099-1112},
	url = {https://iths.pure.elsevier.com/en/publications/object-based-detection-and-classification-of-vehicles-from-high-r},
	language = {English},
	number = {7},
	urldate = {2018-02-07},
	journal = {Photogrammetric Engineering and Remote Sensing},
	author = {Holt, Ashley C. and Seto, Edmund Y. W. and Rivard, Tom and Gong, Peng},
	month = jul,
	year = {2009},
	pages = {871--880},
	file = {Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/8RWKGAKZ/object-based-detection-and-classification-of-vehicles-from-high-r.html:text/html}
}

@article{ammour_deep_2017,
	title = {Deep {Learning} {Approach} for {Car} {Detection} in {UAV} {Imagery}},
	volume = {9},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {http://www.mdpi.com/2072-4292/9/4/312},
	doi = {10.3390/rs9040312},
	abstract = {This paper presents an automatic solution to the problem of detecting and counting cars in unmanned aerial vehicle (UAV) images. This is a challenging task given the very high spatial resolution of UAV images (on the order of a few centimetres) and the extremely high level of detail, which require suitable automatic analysis methods. Our proposed method begins by segmenting the input image into small homogeneous regions, which can be used as candidate locations for car detection. Next, a window is extracted around each region, and deep learning is used to mine highly descriptive features from these windows. We use a deep convolutional neural network (CNN) system that is already pre-trained on huge auxiliary data as a feature extraction tool, combined with a linear support vector machine (SVM) classifier to classify regions into “car” and “no-car” classes. The final step is devoted to a fine-tuning procedure which performs morphological dilation to smooth the detected regions and fill any holes. In addition, small isolated regions are analysed further using a few sliding rectangular windows to locate cars more accurately and remove false positives. To evaluate our method, experiments were conducted on a challenging set of real UAV images acquired over an urban area. The experimental results have proven that the proposed method outperforms the state-of-the-art methods, both in terms of accuracy and computational time.},
	language = {en},
	number = {4},
	urldate = {2018-02-07},
	journal = {Remote Sensing},
	author = {Ammour, Nassim and Alhichri, Haikel and Bazi, Yakoub and Benjdira, Bilel and Alajlan, Naif and Zuair, Mansour},
	month = mar,
	year = {2017},
	keywords = {car counting, convolutional neural networks (CNNs), deep learning, mean-shift segmentation, support vector machines (SVM), UAV imagery},
	pages = {312},
	file = {Full Text PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/TMSDIUUN/Ammour et al. - 2017 - Deep Learning Approach for Car Detection in UAV Im.pdf:application/pdf;Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/JICCERL2/312.html:text/html}
}

@article{luc_semantic_2016,
	title = {Semantic {Segmentation} using {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1611.08408},
	abstract = {Adversarial training has been shown to produce state of the art results for generative image modeling. In this paper we propose an adversarial training approach to train semantic segmentation models. We train a convolutional semantic segmentation network along with an adversarial network that discriminates segmentation maps coming either from the ground truth or from the segmentation network. The motivation for our approach is that it can detect and correct higher-order inconsistencies between ground truth segmentation maps and the ones produced by the segmentation net. Our experiments show that our adversarial training approach leads to improved accuracy on the Stanford Background and PASCAL VOC 2012 datasets.},
	urldate = {2018-02-07},
	journal = {arXiv:1611.08408 [cs]},
	author = {Luc, Pauline and Couprie, Camille and Chintala, Soumith and Verbeek, Jakob},
	month = nov,
	year = {2016},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {arXiv: 1611.08408},
	file = {arXiv\:1611.08408 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/RI5SXRCL/Luc et al. - 2016 - Semantic Segmentation using Adversarial Networks.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/6SQMCH66/1611.html:text/html}
}

@article{audebert_segment-before-detect:_2017,
	title = {Segment-before-{Detect}: {Vehicle} {Detection} and {Classification} through {Semantic} {Segmentation} of {Aerial} {Images}},
	volume = {9},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	shorttitle = {Segment-before-{Detect}},
	url = {http://www.mdpi.com/2072-4292/9/4/368},
	doi = {10.3390/rs9040368},
	abstract = {Like computer vision before, remote sensing has been radically changed by the introduction of deep learning and, more notably, Convolution Neural Networks. Land cover classification, object detection and scene understanding in aerial images rely more and more on deep networks to achieve new state-of-the-art results. Recent architectures such as Fully Convolutional Networks can even produce pixel level annotations for semantic mapping. In this work, we present a deep-learning based segment-before-detect method for segmentation and subsequent detection and classification of several varieties of wheeled vehicles in high resolution remote sensing images. This allows us to investigate object detection and classification on a complex dataset made up of visually similar classes, and to demonstrate the relevance of such a subclass modeling approach. Especially, we want to show that deep learning is also suitable for object-oriented analysis of Earth Observation data as effective object detection can be obtained as a byproduct of accurate semantic segmentation. First, we train a deep fully convolutional network on the ISPRS Potsdam and the NZAM/ONERA Christchurch datasets and show how the learnt semantic maps can be used to extract precise segmentation of vehicles. Then, we show that those maps are accurate enough to perform vehicle detection by simple connected component extraction. This allows us to study the repartition of vehicles in the city. Finally, we train a Convolutional Neural Network to perform vehicle classification on the VEDAI dataset, and transfer its knowledge to classify the individual vehicle instances that we detected.},
	language = {en},
	number = {4},
	urldate = {2018-02-07},
	journal = {Remote Sensing},
	author = {Audebert, Nicolas and Le Saux, Bertrand and Lefèvre, Sébastien},
	month = apr,
	year = {2017},
	keywords = {deep learning, object classification, semantic segmentation, vehicle detection},
	pages = {368},
	file = {Full Text PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/HP9GGA8E/Audebert et al. - 2017 - Segment-before-Detect Vehicle Detection and Class.pdf:application/pdf;Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/SBEU6P7B/368.html:text/html}
}

@article{goodfellow_nips_2016,
	title = {{NIPS} 2016 {Tutorial}: {Generative} {Adversarial} {Networks}},
	shorttitle = {{NIPS} 2016 {Tutorial}},
	url = {http://arxiv.org/abs/1701.00160},
	abstract = {This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises.},
	urldate = {2018-02-07},
	journal = {arXiv:1701.00160 [cs]},
	author = {Goodfellow, Ian},
	month = dec,
	year = {2016},
	keywords = {Computer Science - Learning},
	annote = {arXiv: 1701.00160},
	annote = {Comment: v2-v4 are all typo fixes. No substantive changes relative to v1},
	file = {arXiv\:1701.00160 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/PVI2ESDC/Goodfellow - 2016 - NIPS 2016 Tutorial Generative Adversarial Network.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/9MVXVW52/1701.html:text/html}
}

@article{arbelle_microscopy_2017,
	title = {Microscopy {Cell} {Segmentation} via {Adversarial} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1709.05860},
	abstract = {We present a novel method for cell segmentation in microscopy images which is inspired by the Generative Adversarial Neural Network (GAN) approach. Our framework is built on a pair of two competitive artificial neural networks, with a unique architecture, termed Rib Cage, which are trained simultaneously and together define a min-max game resulting in an accurate segmentation of a given image. Our approach has two main strengths, similar to the GAN, the method does not require a formulation of a loss function for the optimization process. This allows training on a limited amount of annotated data in a weakly supervised manner. Promising segmentation results on real fluorescent microscopy data are presented. The code is freely available at: https://github.com/arbellea/DeepCellSeg.git},
	urldate = {2018-02-07},
	journal = {arXiv:1709.05860 [cs]},
	author = {Arbelle, Assaf and Raviv, Tammy Riklin},
	month = sep,
	year = {2017},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {arXiv: 1709.05860},
	annote = {Comment: Accepted to IEEE International Symposium on Biomedical Imaging (ISBI) 2018},
	file = {arXiv\:1709.05860 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/H2LMAFH4/Arbelle and Raviv - 2017 - Microscopy Cell Segmentation via Adversarial Neura.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/PHB7X6I3/1709.html:text/html}
}

@article{son_retinal_2017,
	title = {Retinal {Vessel} {Segmentation} in {Fundoscopic} {Images} with {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1706.09318},
	abstract = {Retinal vessel segmentation is an indispensable step for automatic detection of retinal diseases with fundoscopic images. Though many approaches have been proposed, existing methods tend to miss fine vessels or allow false positives at terminal branches. Let alone under-segmentation, over-segmentation is also problematic when quantitative studies need to measure the precise width of vessels. In this paper, we present a method that generates the precise map of retinal vessels using generative adversarial training. Our methods achieve dice coefficient of 0.829 on DRIVE dataset and 0.834 on STARE dataset which is the state-of-the-art performance on both datasets.},
	urldate = {2018-02-07},
	journal = {arXiv:1706.09318 [cs]},
	author = {Son, Jaemin and Park, Sang Jun and Jung, Kyu-Hwan},
	month = jun,
	year = {2017},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning},
	annote = {arXiv: 1706.09318},
	annote = {Comment: 9 pages, submitted to DLMIA 2017},
	file = {arXiv\:1706.09318 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/HIVFYU25/Son et al. - 2017 - Retinal Vessel Segmentation in Fundoscopic Images .pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/C88FNJ7M/1706.html:text/html}
}

@book{mundhenk_cars_nodate,
	title = {Cars {Overhead} {With} {Context} {Dataset} at {LLNL}},
	url = {http://gdo-datasci.ucllnl.org/cowc/},
	abstract = {A large diverse set of cars from overhead images, which are useful for training a deep learner to binary classify, detect and count them.},
	urldate = {2018-02-07},
	author = {Mundhenk, T. Nathan},
	file = {Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/5L2XJXU3/cowc.html:text/html}
}

@book{noauthor_2d_nodate,
	title = {2D {Semantic} {Labeling} - {ISPRS}},
	url = {http://www2.isprs.org/commissions/comm3/wg4/semantic-labeling.html},
	urldate = {2018-02-07},
	file = {2D Semantic Labeling - ISPRS:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/NXXU5MPF/semantic-labeling.html:text/html}
}

@book{noauthor_github_nodate,
	title = {{GitHub} - nightrome/really-awesome-gan: {A} list of papers on {Generative} {Adversarial} ({Neural}) {Networks}},
	url = {https://github.com/nightrome/really-awesome-gan},
	urldate = {2018-02-07},
	file = {GitHub - nightrome/really-awesome-gan\: A list of papers on Generative Adversarial (Neural) Networks:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/DEPJNPUI/really-awesome-gan.html:text/html}
}

@article{razakarivony_vehicle_2015,
	title = {Vehicle {Detection} in {Aerial} {Imagery} : {A} small target detection benchmark},
	shorttitle = {Vehicle {Detection} in {Aerial} {Imagery}},
	url = {https://hal.archives-ouvertes.fr/hal-01122605},
	abstract = {This paper introduces VEDAI: Vehicle Detection in Aerial Imagery a new database of aerial images provided as a tool to benchmark automatic target recognition algorithms in unconstrained environments. The vehicles contained in the database, in addition of being small, exhibit different variabil-ities such as multiple orientations, lighting/shadowing changes, specularities or occlusions. Furthermore, each image is available in several spectral bands and resolutions. A precise experimental protocol is also given, ensuring that the experimental results obtained by different people can be properly reproduce and compared. Finally, the paper also gives the performance of baseline algorithms on this dataset, for different settings of these algorithms, to illustrate the difficulties of the task and provide baseline comparisons.},
	urldate = {2018-02-07},
	journal = {Journal of Visual Communication and Image Representation, Elsevier},
	author = {Razakarivony, Sébastien and Jurie, Frédéric},
	month = mar,
	year = {2015},
	keywords = {Aerial Imagery, Database, Detection, Infrared Imagery, Low Resolution Images, Vehicles},
	file = {HAL PDF Full Text:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/RVTQZ6AR/Razakarivony and Jurie - 2015 - Vehicle Detection in Aerial Imagery  A small targ.pdf:application/pdf}
}

@article{reed_generative_2016,
	title = {Generative {Adversarial} {Text} to {Image} {Synthesis}},
	url = {https://arxiv.org/abs/1605.05396},
	language = {en},
	urldate = {2018-02-18},
	author = {Reed, Scott and Akata, Zeynep and Yan, Xinchen and Logeswaran, Lajanugen and Schiele, Bernt and Lee, Honglak},
	month = may,
	year = {2016}
}

@article{mirza_conditional_2014,
	title = {Conditional {Generative} {Adversarial} {Nets}},
	url = {http://arxiv.org/abs/1411.1784},
	abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
	urldate = {2018-02-18},
	journal = {arXiv:1411.1784 [cs, stat]},
	author = {Mirza, Mehdi and Osindero, Simon},
	month = nov,
	year = {2014},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	annote = {arXiv: 1411.1784},
	file = {arXiv\:1411.1784 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/EJVAH8GV/Mirza and Osindero - 2014 - Conditional Generative Adversarial Nets.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/4T58M3FP/1411.html:text/html}
}

@article{isola_image--image_2016,
	title = {Image-to-{Image} {Translation} with {Conditional} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1611.07004},
	abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
	urldate = {2018-02-18},
	journal = {arXiv:1611.07004 [cs]},
	author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
	month = nov,
	year = {2016},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {arXiv: 1611.07004},
	annote = {Comment: Website: https://phillipi.github.io/pix2pix/},
	file = {arXiv\:1611.07004 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/D7UBSIIJ/Isola et al. - 2016 - Image-to-Image Translation with Conditional Advers.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/7649YWA9/1611.html:text/html}
}

@article{zhong_robust_2017,
	title = {Robust {Vehicle} {Detection} in {Aerial} {Images} {Based} on {Cascaded} {Convolutional} {Neural} {Networks}},
	volume = {17},
	issn = {1424-8220},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5751529/},
	doi = {10.3390/s17122720},
	abstract = {Vehicle detection in aerial images is an important and challenging task. Traditionally, many target detection models based on sliding-window fashion were developed and achieved acceptable performance, but these models are time-consuming in the detection phase. Recently, with the great success of convolutional neural networks (CNNs) in computer vision, many state-of-the-art detectors have been designed based on deep CNNs. However, these CNN-based detectors are inefficient when applied in aerial image data due to the fact that the existing CNN-based models struggle with small-size object detection and precise localization. To improve the detection accuracy without decreasing speed, we propose a CNN-based detection model combining two independent convolutional neural networks, where the first network is applied to generate a set of vehicle-like regions from multi-feature maps of different hierarchies and scales. Because the multi-feature maps combine the advantage of the deep and shallow convolutional layer, the first network performs well on locating the small targets in aerial image data. Then, the generated candidate regions are fed into the second network for feature extraction and decision making. Comprehensive experiments are conducted on the Vehicle Detection in Aerial Imagery (VEDAI) dataset and Munich vehicle dataset. The proposed cascaded detection model yields high performance, not only in detection accuracy but also in detection speed.},
	number = {12},
	urldate = {2018-02-18},
	journal = {Sensors (Basel, Switzerland)},
	author = {Zhong, Jiandan and Lei, Tao and Yao, Guangle},
	month = nov,
	year = {2017},
	pmid = {29186756},
	pmcid = {PMC5751529},
	file = {PubMed Central Full Text PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/TWVTMQQV/Zhong et al. - 2017 - Robust Vehicle Detection in Aerial Images Based on.pdf:application/pdf}
}

@article{pathak_context_2016,
	title = {Context {Encoders}: {Feature} {Learning} by {Inpainting}},
	shorttitle = {Context {Encoders}},
	url = {http://arxiv.org/abs/1604.07379},
	abstract = {We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders – a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods.},
	urldate = {2018-02-19},
	journal = {arXiv:1604.07379 [cs]},
	author = {Pathak, Deepak and Krahenbuhl, Philipp and Donahue, Jeff and Darrell, Trevor and Efros, Alexei A.},
	month = apr,
	year = {2016},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Artificial Intelligence, Computer Science - Graphics},
	annote = {arXiv: 1604.07379},
	annote = {Comment: New results on ImageNet Generation},
	file = {arXiv\:1604.07379 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/2ZB2G9J4/Pathak et al. - 2016 - Context Encoders Feature Learning by Inpainting.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/V5NV93VR/1604.html:text/html}
}

@article{larsen_autoencoding_2015,
	title = {Autoencoding beyond pixels using a learned similarity metric},
	url = {http://arxiv.org/abs/1512.09300},
	abstract = {We present an autoencoder that leverages learned representations to better measure similarities in data space. By combining a variational autoencoder with a generative adversarial network we can use learned feature representations in the GAN discriminator as basis for the VAE reconstruction objective. Thereby, we replace element-wise errors with feature-wise errors to better capture the data distribution while offering invariance towards e.g. translation. We apply our method to images of faces and show that it outperforms VAEs with element-wise similarity measures in terms of visual fidelity. Moreover, we show that the method learns an embedding in which high-level abstract visual features (e.g. wearing glasses) can be modified using simple arithmetic.},
	urldate = {2018-02-19},
	journal = {arXiv:1512.09300 [cs, stat]},
	author = {Larsen, Anders Boesen Lindbo and Sønderby, Søren Kaae and Larochelle, Hugo and Winther, Ole},
	month = dec,
	year = {2015},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Statistics - Machine Learning},
	annote = {arXiv: 1512.09300},
	file = {arXiv\:1512.09300 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/3BFHC3R9/Larsen et al. - 2015 - Autoencoding beyond pixels using a learned similar.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/H8DPY2DA/1512.html:text/html}
}

@article{shelhamer_fully_2016,
	title = {Fully {Convolutional} {Networks} for {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/1605.06211},
	abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, improve on the previous best result in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves improved segmentation of PASCAL VOC (30\% relative improvement to 67.2\% mean IU on 2012), NYUDv2, SIFT Flow, and PASCAL-Context, while inference takes one tenth of a second for a typical image.},
	urldate = {2018-02-19},
	journal = {arXiv:1605.06211 [cs]},
	author = {Shelhamer, Evan and Long, Jonathan and Darrell, Trevor},
	month = may,
	year = {2016},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {arXiv: 1605.06211},
	annote = {Comment: to appear in PAMI (accepted May, 2016); journal edition of arXiv:1411.4038},
	file = {arXiv\:1605.06211 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/3GXW77PR/Shelhamer et al. - 2016 - Fully Convolutional Networks for Semantic Segmenta.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/9VFLNBJ7/1605.html:text/html}
}

@article{ronneberger_u-net:_2015,
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	shorttitle = {U-{Net}},
	url = {http://arxiv.org/abs/1505.04597},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	urldate = {2018-02-19},
	journal = {arXiv:1505.04597 [cs]},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	month = may,
	year = {2015},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {arXiv: 1505.04597},
	annote = {Comment: conditionally accepted at MICCAI 2015},
	file = {arXiv\:1505.04597 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/FIGRWAJA/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/U26I2DFR/1505.html:text/html}
}

@article{rezaei_conditional_2017,
	title = {Conditional {Adversarial} {Network} for {Semantic} {Segmentation} of {Brain} {Tumor}},
	url = {http://arxiv.org/abs/1708.05227},
	abstract = {Automated medical image analysis has a significant value in diagnosis and treatment of lesions. Brain tumors segmentation has a special importance and difficulty due to the difference in appearances and shapes of the different tumor regions in magnetic resonance images. Additionally, the data sets are heterogeneous and usually limited in size in comparison with the computer vision problems. The recently proposed adversarial training has shown promising results in generative image modeling. In this paper, we propose a novel end-to-end trainable architecture for brain tumor semantic segmentation through conditional adversarial training. We exploit conditional Generative Adversarial Network (cGAN) and train a semantic segmentation Convolution Neural Network (CNN) along with an adversarial network that discriminates segmentation maps coming from the ground truth or from the segmentation network for BraTS 2017 segmentation task[15, 4, 2, 3]. We also propose an end-to-end trainable CNN for survival day prediction based on deep learning techniques for BraTS 2017 prediction task [15, 4, 2, 3]. The experimental results demonstrate the superior ability of the proposed approach for both tasks. The proposed model achieves on validation data a DICE score, Sensitivity and Specificity respectively 0.68, 0.99 and 0.98 for the whole tumor, regarding online judgment system.},
	urldate = {2018-02-19},
	journal = {arXiv:1708.05227 [cs]},
	author = {Rezaei, Mina and Harmuth, Konstantin and Gierke, Willi and Kellermeier, Thomas and Fischer, Martin and Yang, Haojin and Meinel, Christoph},
	month = aug,
	year = {2017},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {arXiv: 1708.05227},
	annote = {Comment: Submitted to BraTS challenges which is part of MICCAI-2017},
	file = {arXiv\:1708.05227 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/L42KJWG7/Rezaei et al. - 2017 - Conditional Adversarial Network for Semantic Segme.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/Z8S283N3/1708.html:text/html}
}

@article{yang_automatic_2017,
	title = {Automatic {Liver} {Segmentation} {Using} an {Adversarial} {Image}-to-{Image} {Network}},
	url = {http://arxiv.org/abs/1707.08037},
	abstract = {Automatic liver segmentation in 3D medical images is essential in many clinical applications, such as pathological diagnosis of hepatic diseases, surgical planning, and postoperative assessment. However, it is still a very challenging task due to the complex background, fuzzy boundary, and various appearance of liver. In this paper, we propose an automatic and efficient algorithm to segment liver from 3D CT volumes. A deep image-to-image network (DI2IN) is first deployed to generate the liver segmentation, employing a convolutional encoder-decoder architecture combined with multi-level feature concatenation and deep supervision. Then an adversarial network is utilized during training process to discriminate the output of DI2IN from ground truth, which further boosts the performance of DI2IN. The proposed method is trained on an annotated dataset of 1000 CT volumes with various different scanning protocols (e.g., contrast and non-contrast, various resolution and position) and large variations in populations (e.g., ages and pathology). Our approach outperforms the state-of-the-art solutions in terms of segmentation accuracy and computing efficiency.},
	urldate = {2018-02-19},
	journal = {arXiv:1707.08037 [cs]},
	author = {Yang, Dong and Xu, Daguang and Zhou, S. Kevin and Georgescu, Bogdan and Chen, Mingqing and Grbic, Sasa and Metaxas, Dimitris and Comaniciu, Dorin},
	month = jul,
	year = {2017},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {arXiv: 1707.08037},
	annote = {Comment: Accepted by MICCAI 2017},
	file = {arXiv\:1707.08037 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/S8IMP6KS/Yang et al. - 2017 - Automatic Liver Segmentation Using an Adversarial .pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/6TFBL573/1707.html:text/html}
}

@article{zheng_conditional_2015,
	title = {Conditional {Random} {Fields} as {Recurrent} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1502.03240},
	doi = {10.1109/ICCV.2015.179},
	abstract = {Pixel-level labelling tasks, such as semantic segmentation, play a central role in image understanding. Recent approaches have attempted to harness the capabilities of deep learning techniques for image recognition to tackle pixel-level labelling tasks. One central issue in this methodology is the limited capacity of deep learning techniques to delineate visual objects. To solve this problem, we introduce a new form of convolutional neural network that combines the strengths of Convolutional Neural Networks (CNNs) and Conditional Random Fields (CRFs)-based probabilistic graphical modelling. To this end, we formulate mean-field approximate inference for the Conditional Random Fields with Gaussian pairwise potentials as Recurrent Neural Networks. This network, called CRF-RNN, is then plugged in as a part of a CNN to obtain a deep network that has desirable properties of both CNNs and CRFs. Importantly, our system fully integrates CRF modelling with CNNs, making it possible to train the whole deep network end-to-end with the usual back-propagation algorithm, avoiding offline post-processing methods for object delineation. We apply the proposed method to the problem of semantic image segmentation, obtaining top results on the challenging Pascal VOC 2012 segmentation benchmark.},
	urldate = {2018-02-20},
	journal = {arXiv:1502.03240 [cs]},
	author = {Zheng, Shuai and Jayasumana, Sadeep and Romera-Paredes, Bernardino and Vineet, Vibhav and Su, Zhizhong and Du, Dalong and Huang, Chang and Torr, Philip H. S.},
	month = dec,
	year = {2015},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {1529--1537},
	annote = {arXiv: 1502.03240},
	annote = {Comment: This paper is published in IEEE ICCV 2015},
	file = {arXiv\:1502.03240 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/TZAVRFQR/Zheng et al. - 2015 - Conditional Random Fields as Recurrent Neural Netw.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/R3RUEQ7M/1502.html:text/html}
}

@article{schwing_fully_2015,
	title = {Fully {Connected} {Deep} {Structured} {Networks}},
	url = {http://arxiv.org/abs/1503.02351},
	abstract = {Convolutional neural networks with many layers have recently been shown to achieve excellent results on many high-level tasks such as image classification, object detection and more recently also semantic segmentation. Particularly for semantic segmentation, a two-stage procedure is often employed. Hereby, convolutional networks are trained to provide good local pixel-wise features for the second step being traditionally a more global graphical model. In this work we unify this two-stage process into a single joint training algorithm. We demonstrate our method on the semantic image segmentation task and show encouraging results on the challenging PASCAL VOC 2012 dataset.},
	urldate = {2018-02-20},
	journal = {arXiv:1503.02351 [cs]},
	author = {Schwing, Alexander G. and Urtasun, Raquel},
	month = mar,
	year = {2015},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning},
	annote = {arXiv: 1503.02351},
	file = {arXiv\:1503.02351 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/3ZPBJEJ8/Schwing and Urtasun - 2015 - Fully Connected Deep Structured Networks.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/I8E97LWB/1503.html:text/html}
}

@article{arnab_higher_2015,
	title = {Higher {Order} {Conditional} {Random} {Fields} in {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1511.08119},
	abstract = {We address the problem of semantic segmentation using deep learning. Most segmentation systems include a Conditional Random Field (CRF) to produce a structured output that is consistent with the image's visual features. Recent deep learning approaches have incorporated CRFs into Convolutional Neural Networks (CNNs), with some even training the CRF end-to-end with the rest of the network. However, these approaches have not employed higher order potentials, which have previously been shown to significantly improve segmentation performance. In this paper, we demonstrate that two types of higher order potential, based on object detections and superpixels, can be included in a CRF embedded within a deep network. We design these higher order potentials to allow inference with the differentiable mean field algorithm. As a result, all the parameters of our richer CRF model can be learned end-to-end with our pixelwise CNN classifier. We achieve state-of-the-art segmentation performance on the PASCAL VOC benchmark with these trainable higher order potentials.},
	urldate = {2018-02-20},
	journal = {arXiv:1511.08119 [cs]},
	author = {Arnab, Anurag and Jayasumana, Sadeep and Zheng, Shuai and Torr, Philip},
	month = nov,
	year = {2015},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {arXiv: 1511.08119},
	annote = {Comment: ECCV 2016},
	file = {arXiv\:1511.08119 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/BVRDDWTM/Arnab et al. - 2015 - Higher Order Conditional Random Fields in Deep Neu.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/M3PCFXCZ/1511.html:text/html}
}

@article{goodfellow_generative_2014,
	title = {Generative {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1406.2661},
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	urldate = {2018-02-20},
	journal = {arXiv:1406.2661 [cs, stat]},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	month = jun,
	year = {2014},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	annote = {arXiv: 1406.2661},
	file = {arXiv\:1406.2661 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/TKWRD4TK/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/5QYEN4TS/1406.html:text/html}
}

@article{pinheiro_learning_2016,
	title = {Learning to {Refine} {Object} {Segments}},
	url = {http://arxiv.org/abs/1603.08695},
	abstract = {Object segmentation requires both object-level information and low-level pixel data. This presents a challenge for feedforward networks: lower layers in convolutional nets capture rich spatial information, while upper layers encode object-level knowledge but are invariant to factors such as pose and appearance. In this work we propose to augment feedforward nets for object segmentation with a novel top-down refinement approach. The resulting bottom-up/top-down architecture is capable of efficiently generating high-fidelity object masks. Similarly to skip connections, our approach leverages features at all layers of the net. Unlike skip connections, our approach does not attempt to output independent predictions at each layer. Instead, we first output a coarse `mask encoding' in a feedforward pass, then refine this mask encoding in a top-down pass utilizing features at successively lower layers. The approach is simple, fast, and effective. Building on the recent DeepMask network for generating object proposals, we show accuracy improvements of 10-20\% in average recall for various setups. Additionally, by optimizing the overall network architecture, our approach, which we call SharpMask, is 50\% faster than the original DeepMask network (under .8s per image).},
	urldate = {2018-02-20},
	journal = {arXiv:1603.08695 [cs]},
	author = {Pinheiro, Pedro O. and Lin, Tsung-Yi and Collobert, Ronan and Dollàr, Piotr},
	month = mar,
	year = {2016},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {arXiv: 1603.08695},
	annote = {Comment: extended version of ECCV camera-ready (figures 6-9 only in arXiv)},
	file = {arXiv\:1603.08695 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/IR98NY9M/Pinheiro et al. - 2016 - Learning to Refine Object Segments.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/ETBXXT97/1603.html:text/html}
}

@article{li_deepunet:_2017,
	title = {{DeepUNet}: {A} {Deep} {Fully} {Convolutional} {Network} for {Pixel}-level {Sea}-{Land} {Segmentation}},
	shorttitle = {{DeepUNet}},
	url = {http://arxiv.org/abs/1709.00201},
	abstract = {Semantic segmentation is a fundamental research in remote sensing image processing. Because of the complex maritime environment, the sea-land segmentation is a challenging task. Although the neural network has achieved excellent performance in semantic segmentation in the last years, there are a few of works using CNN for sea-land segmentation and the results could be further improved. This paper proposes a novel deep convolution neural network named DeepUNet. Like the U-Net, its structure has a contracting path and an expansive path to get high resolution output. But differently, the DeepUNet uses DownBlocks instead of convolution layers in the contracting path and uses UpBlock in the expansive path. The two novel blocks bring two new connections that are U-connection and Plus connection. They are promoted to get more precise segmentation results. To verify our network architecture, we made a new challenging sea-land dataset and compare the DeepUNet on it with the SegNet and the U-Net. Experimental results show that DeepUNet achieved good performance compared with other architectures, especially in high-resolution remote sensing imagery.},
	urldate = {2018-03-08},
	journal = {arXiv:1709.00201 [cs]},
	author = {Li, Ruirui and Liu, Wenjie and Yang, Lei and Sun, Shihao and Hu, Wei and Zhang, Fan and Li, Wei},
	month = sep,
	year = {2017},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {arXiv: 1709.00201},
	file = {arXiv\:1709.00201 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/9ML2X8ES/Li et al. - 2017 - DeepUNet A Deep Fully Convolutional Network for P.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/M3CVZC8T/1709.html:text/html}
}

@article{yu_unitbox:_2016,
	title = {{UnitBox}: {An} {Advanced} {Object} {Detection} {Network}},
	shorttitle = {{UnitBox}},
	url = {http://arxiv.org/abs/1608.01471},
	doi = {10.1145/2964284.2967274},
	urldate = {2018-03-08},
	journal = {arXiv:1608.01471 [cs]},
	author = {Yu, Jiahui and Jiang, Yuning and Wang, Zhangyang and Cao, Zhimin and Huang, Thomas},
	year = {2016},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, 68U01, I.4.0},
	pages = {516--520},
	annote = {arXiv: 1608.01471},
	annote = {Comment: To appear in ACM MM 2016, 5 pages, 6 figures},
	file = {arXiv\:1608.01471 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/KKEPG7U7/Yu et al. - 2016 - UnitBox An Advanced Object Detection Network.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/UGSNGMVS/1608.html:text/html}
}

@article{sudre_generalised_2017,
	title = {Generalised {Dice} overlap as a deep learning loss function for highly unbalanced segmentations},
	volume = {10553},
	url = {http://arxiv.org/abs/1707.03237},
	doi = {10.1007/978-3-319-67558-9_28},
	abstract = {Deep-learning has proved in recent years to be a powerful tool for image analysis and is now widely used to segment both 2D and 3D medical images. Deep-learning segmentation frameworks rely not only on the choice of network architecture but also on the choice of loss function. When the segmentation process targets rare observations, a severe class imbalance is likely to occur between candidate labels, thus resulting in sub-optimal performance. In order to mitigate this issue, strategies such as the weighted cross-entropy function, the sensitivity function or the Dice loss function, have been proposed. In this work, we investigate the behavior of these loss functions and their sensitivity to learning rate tuning in the presence of different rates of label imbalance across 2D and 3D segmentation tasks. We also propose to use the class re-balancing properties of the Generalized Dice overlap, a known metric for segmentation assessment, as a robust and accurate deep-learning loss function for unbalanced tasks.},
	urldate = {2018-03-08},
	journal = {arXiv:1707.03237 [cs]},
	author = {Sudre, Carole H. and Li, Wenqi and Vercauteren, Tom and Ourselin, Sébastien and Cardoso, M. Jorge},
	year = {2017},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {240--248},
	annote = {arXiv: 1707.03237},
	file = {arXiv\:1707.03237 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/A5ICEQRP/Sudre et al. - 2017 - Generalised Dice overlap as a deep learning loss f.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/PD3APLAE/1707.html:text/html}
}

@inproceedings{rahman_optimizing_2016,
	address = {Cham},
	title = {Optimizing {Intersection}-{Over}-{Union} in {Deep} {Neural} {Networks} for {Image} {Segmentation}},
	isbn = {978-3-319-50835-1},
	abstract = {We consider the problem of learning deep neural networks (DNNs) for object category segmentation, where the goal is to label each pixel in an image as being part of a given object (foreground) or not (background). Deep neural networks are usually trained with simple loss functions (e.g., softmax loss). These loss functions are appropriate for standard classification problems where the performance is measured by the overall classification accuracy. For object category segmentation, the two classes (foreground and background) are very imbalanced. The intersection-over-union (IoU) is usually used to measure the performance of any object category segmentation method. In this paper, we propose an approach for directly optimizing this IoU measure in deep neural networks. Our experimental results on two object category segmentation datasets demonstrate that our approach outperforms DNNs trained with standard softmax loss.},
	booktitle = {Advances in {Visual} {Computing}},
	publisher = {Springer International Publishing},
	author = {Rahman, Md Atiqur and Wang, Yang},
	editor = {Bebis, George and Boyle, Richard and Parvin, Bahram and Koracin, Darko and Porikli, Fatih and Skaff, Sandra and Entezari, Alireza and Min, Jianyuan and Iwai, Daisuke and Sadagic, Amela and Scheidegger, Carlos and Isenberg, Tobias},
	year = {2016},
	pages = {234--244}
}

@article{simonyan_very_2014,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	urldate = {2018-03-08},
	journal = {arXiv:1409.1556 [cs]},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = sep,
	year = {2014},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {arXiv: 1409.1556},
	file = {arXiv\:1409.1556 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/FJKKILH9/Simonyan and Zisserman - 2014 - Very Deep Convolutional Networks for Large-Scale I.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/5V9BWMVH/1409.html:text/html}
}

@article{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2018-03-08},
	journal = {arXiv:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {arXiv: 1512.03385},
	annote = {Comment: Tech report},
	file = {arXiv\:1512.03385 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/KYLNHKIV/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/NEUQVUCP/1512.html:text/html}
}

@article{ren_faster_2015,
	title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
	shorttitle = {Faster {R}-{CNN}},
	url = {http://arxiv.org/abs/1506.01497},
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
	urldate = {2018-03-08},
	journal = {arXiv:1506.01497 [cs]},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	month = jun,
	year = {2015},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {arXiv: 1506.01497},
	annote = {Comment: Extended tech report},
	file = {arXiv\:1506.01497 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/ZGMJTQXH/Ren et al. - 2015 - Faster R-CNN Towards Real-Time Object Detection w.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/EKULLNE3/1506.html:text/html}
}

@article{girshick_fast_2015,
	title = {Fast {R}-{CNN}},
	url = {http://arxiv.org/abs/1504.08083},
	abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
	urldate = {2018-03-08},
	journal = {arXiv:1504.08083 [cs]},
	author = {Girshick, Ross},
	month = apr,
	year = {2015},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {arXiv: 1504.08083},
	annote = {Comment: To appear in ICCV 2015},
	file = {arXiv\:1504.08083 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/FX9IJ6QG/Girshick - 2015 - Fast R-CNN.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/4QNPPRG2/1504.html:text/html}
}

@article{badrinarayanan_segnet:_2015,
	title = {{SegNet}: {A} {Deep} {Convolutional} {Encoder}-{Decoder} {Architecture} for {Image} {Segmentation}},
	shorttitle = {{SegNet}},
	url = {http://arxiv.org/abs/1511.00561},
	abstract = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN and also with the well known DeepLab-LargeFOV, DeconvNet architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. We show that SegNet provides good performance with competitive inference time and more efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.},
	urldate = {2018-03-08},
	journal = {arXiv:1511.00561 [cs]},
	author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
	month = nov,
	year = {2015},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {arXiv: 1511.00561},
	file = {arXiv\:1511.00561 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/Y5ZWWST6/Badrinarayanan et al. - 2015 - SegNet A Deep Convolutional Encoder-Decoder Archi.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/WBUKLZZJ/1511.html:text/html}
}

@article{szegedy_inception-v4_2016,
	title = {Inception-v4, {Inception}-{ResNet} and the {Impact} of {Residual} {Connections} on {Learning}},
	url = {http://arxiv.org/abs/1602.07261},
	abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge},
	urldate = {2018-03-08},
	journal = {arXiv:1602.07261 [cs]},
	author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alex},
	month = feb,
	year = {2016},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {arXiv: 1602.07261},
	file = {arXiv\:1602.07261 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/UUVTW5C9/Szegedy et al. - 2016 - Inception-v4, Inception-ResNet and the Impact of R.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/46MZY8CD/1602.html:text/html}
}

@article{zagoruyko_wide_2016,
	title = {Wide {Residual} {Networks}},
	url = {http://arxiv.org/abs/1605.07146},
	abstract = {Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on ImageNet. Our code and models are available at https://github.com/szagoruyko/wide-residual-networks},
	urldate = {2018-03-08},
	journal = {arXiv:1605.07146 [cs]},
	author = {Zagoruyko, Sergey and Komodakis, Nikos},
	month = may,
	year = {2016},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {arXiv: 1605.07146},
	file = {arXiv\:1605.07146 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/I3PAU8H6/Zagoruyko and Komodakis - 2016 - Wide Residual Networks.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/3JJU67IJ/1605.html:text/html}
}

@article{wu_wider_2016,
	title = {Wider or {Deeper}: {Revisiting} the {ResNet} {Model} for {Visual} {Recognition}},
	shorttitle = {Wider or {Deeper}},
	url = {http://arxiv.org/abs/1611.10080},
	abstract = {The trend towards increasingly deep neural networks has been driven by a general observation that increasing depth increases the performance of a network. Recently, however, evidence has been amassing that simply increasing depth may not be the best way to increase performance, particularly given other limitations. Investigations into deep residual networks have also suggested that they may not in fact be operating as a single deep network, but rather as an ensemble of many relatively shallow networks. We examine these issues, and in doing so arrive at a new interpretation of the unravelled view of deep residual networks which explains some of the behaviours that have been observed experimentally. As a result, we are able to derive a new, shallower, architecture of residual networks which significantly outperforms much deeper models such as ResNet-200 on the ImageNet classification dataset. We also show that this performance is transferable to other problem domains by developing a semantic segmentation approach which outperforms the state-of-the-art by a remarkable margin on datasets including PASCAL VOC, PASCAL Context, and Cityscapes. The architecture that we propose thus outperforms its comparators, including very deep ResNets, and yet is more efficient in memory use and sometimes also in training time. The code and models are available at https://github.com/itijyou/ademxapp},
	urldate = {2018-03-08},
	journal = {arXiv:1611.10080 [cs]},
	author = {Wu, Zifeng and Shen, Chunhua and Hengel, Anton van den},
	month = nov,
	year = {2016},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {arXiv: 1611.10080},
	annote = {Comment: Code available at: https://github.com/itijyou/ademxapp},
	file = {arXiv\:1611.10080 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/92RHUXNP/Wu et al. - 2016 - Wider or Deeper Revisiting the ResNet Model for V.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/23HX8GYH/1611.html:text/html}
}

@article{he_identity_2016,
	title = {Identity {Mappings} in {Deep} {Residual} {Networks}},
	url = {http://arxiv.org/abs/1603.05027},
	abstract = {Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62\% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers},
	urldate = {2018-03-08},
	journal = {arXiv:1603.05027 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = mar,
	year = {2016},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning},
	annote = {arXiv: 1603.05027},
	annote = {Comment: ECCV 2016 camera-ready},
	file = {arXiv\:1603.05027 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/2S6BJ97Q/He et al. - 2016 - Identity Mappings in Deep Residual Networks.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/GGVI3ADW/1603.html:text/html}
}

@article{sermanet_overfeat:_2013,
	title = {{OverFeat}: {Integrated} {Recognition}, {Localization} and {Detection} using {Convolutional} {Networks}},
	shorttitle = {{OverFeat}},
	url = {http://arxiv.org/abs/1312.6229},
	abstract = {We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.},
	urldate = {2018-03-09},
	journal = {arXiv:1312.6229 [cs]},
	author = {Sermanet, Pierre and Eigen, David and Zhang, Xiang and Mathieu, Michael and Fergus, Rob and LeCun, Yann},
	month = dec,
	year = {2013},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {arXiv: 1312.6229},
	file = {arXiv\:1312.6229 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/IRDJPFAA/Sermanet et al. - 2013 - OverFeat Integrated Recognition, Localization and.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/IVUA26S2/1312.html:text/html}
}

@article{simonyan_two-stream_2014,
	title = {Two-{Stream} {Convolutional} {Networks} for {Action} {Recognition} in {Videos}},
	url = {http://arxiv.org/abs/1406.2199},
	abstract = {We investigate architectures of discriminatively trained deep Convolutional Networks (ConvNets) for action recognition in video. The challenge is to capture the complementary information on appearance from still frames and motion between frames. We also aim to generalise the best performing hand-crafted features within a data-driven learning framework. Our contribution is three-fold. First, we propose a two-stream ConvNet architecture which incorporates spatial and temporal networks. Second, we demonstrate that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data. Finally, we show that multi-task learning, applied to two different action classification datasets, can be used to increase the amount of training data and improve the performance on both. Our architecture is trained and evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51, where it is competitive with the state of the art. It also exceeds by a large margin previous attempts to use deep nets for video classification.},
	urldate = {2018-03-09},
	journal = {arXiv:1406.2199 [cs]},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = jun,
	year = {2014},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {arXiv: 1406.2199},
	file = {arXiv\:1406.2199 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/7KTN5QWU/Simonyan and Zisserman - 2014 - Two-Stream Convolutional Networks for Action Recog.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/5SL2N9FP/1406.html:text/html}
}

@incollection{krizhevsky_imagenet_2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 25},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
	year = {2012},
	pages = {1097--1105}
}

@article{lin_microsoft_2014,
	title = {Microsoft {COCO}: {Common} {Objects} in {Context}},
	shorttitle = {Microsoft {COCO}},
	url = {http://arxiv.org/abs/1405.0312},
	abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
	urldate = {2018-03-09},
	journal = {arXiv:1405.0312 [cs]},
	author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Dollár, Piotr},
	month = may,
	year = {2014},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {arXiv: 1405.0312},
	annote = {Comment: 1) updated annotation pipeline description and figures; 2) added new section describing datasets splits; 3) updated author list},
	file = {arXiv\:1405.0312 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/CMIUKM8Q/Lin et al. - 2014 - Microsoft COCO Common Objects in Context.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/P2I3IWKA/1405.html:text/html}
}

@techreport{krizhevsky_learning_2009,
	title = {Learning multiple layers of features from tiny images},
	abstract = {Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it difficult to learn a good set of filters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is significantly},
	author = {Krizhevsky, Alex},
	year = {2009},
	file = {Citeseer - Full Text PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/GRQQTTMC/Krizhevsky - 2009 - Learning multiple layers of features from tiny ima.pdf:application/pdf;Citeseer - Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/E4DKKNTF/summary.html:text/html}
}

@article{garcia-garcia_review_2017,
	title = {A {Review} on {Deep} {Learning} {Techniques} {Applied} to {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/1704.06857},
	abstract = {Image semantic segmentation is more and more being of interest for computer vision and machine learning researchers. Many applications on the rise need accurate and efficient segmentation mechanisms: autonomous driving, indoor navigation, and even virtual or augmented reality systems to name a few. This demand coincides with the rise of deep learning approaches in almost every field or application target related to computer vision, including semantic segmentation or scene understanding. This paper provides a review on deep learning methods for semantic segmentation applied to various application areas. Firstly, we describe the terminology of this field as well as mandatory background concepts. Next, the main datasets and challenges are exposed to help researchers decide which are the ones that best suit their needs and their targets. Then, existing methods are reviewed, highlighting their contributions and their significance in the field. Finally, quantitative results are given for the described methods and the datasets in which they were evaluated, following up with a discussion of the results. At last, we point out a set of promising future works and draw our own conclusions about the state of the art of semantic segmentation using deep learning techniques.},
	urldate = {2018-03-09},
	journal = {arXiv:1704.06857 [cs]},
	author = {Garcia-Garcia, Alberto and Orts-Escolano, Sergio and Oprea, Sergiu and Villena-Martinez, Victor and Garcia-Rodriguez, Jose},
	month = apr,
	year = {2017},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	annote = {arXiv: 1704.06857},
	annote = {Comment: Submitted to TPAMI on Apr. 22, 2017},
	file = {arXiv\:1704.06857 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/8JT32VLC/Garcia-Garcia et al. - 2017 - A Review on Deep Learning Techniques Applied to Se.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/K6YLNAU7/1704.html:text/html}
}

@article{zhao_classification_2007,
	title = {Classification of {High} {Spatial} {Resolution} {Imagery} {Using} {Improved} {Gaussian} {Markov} {Random}-{Field}-{Based} {Texture} {Features}},
	volume = {45},
	issn = {0196-2892},
	doi = {10.1109/TGRS.2007.892602},
	abstract = {Gaussian Markov random fields (GMRFs) are used to analyze textures. GMRFs measure the interdependence of neighboring pixels within a texture to produce features. In this paper, neighboring pixels are taken into account in a priority sequence according to their distance from the center pixel, and a step-by-step least squares method is proposed to extract a novel set of GMRF texture features, named as PS-GMRF. A complete procedure is first designed to classify texture samples of QuickBird imagery. After texture feature extraction, a subset of PS-GMRF features is obtained by the sequential floating forward-selection method. Then, the maximum a posteriori iterated conditional mode classification algorithm is used, involving the selected PS-GMRF texture features in combination with spectral features. The experimental results show that the performance of classifying texture samples on high spatial resolution QuickBird satellite imagery is improved when texture features and spectral features are used jointly, and PS-GMRF features have a higher discrimination power compared to the classical GMRF features, making a notable improvement in classification accuracy from 71.84\% to 94.01\%. On the other hand, it is found that one of the PS-GMRF texture features - the lowest order variance - is effective for residential-area detection. Some results for IKONOS and SPOT-5 images show that the integration of the lowest order variance with spectral features improves the classification accuracy compared to classification with purely spectral features},
	number = {5},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Zhao, Y. and Zhang, L. and Li, P. and Huang, B.},
	month = may,
	year = {2007},
	keywords = {Classifying texture samples, Feature extraction, Gaussian Markov random field, Gaussian Markov random fields (GMRFs), IKONOS images, Image analysis, image classification, image texture, Image texture analysis, Laboratories, least squares (LS) method, least squares method, Least squares methods, Markov random fields, Pixel, priority sequence, PS-GMRF features, QuickBird images, remote sensing, Remote sensing, residential-area detection, Satellites, Spatial resolution, SPOT-5 images, texture feature},
	pages = {1458--1468},
	file = {IEEE Xplore Abstract Record:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/7BNDLT76/4156351.html:text/html}
}

@article{ledig_photo-realistic_2016,
	title = {Photo-{Realistic} {Single} {Image} {Super}-{Resolution} {Using} a {Generative} {Adversarial} {Network}},
	url = {http://arxiv.org/abs/1609.04802},
	abstract = {Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.},
	urldate = {2018-03-09},
	journal = {arXiv:1609.04802 [cs, stat]},
	author = {Ledig, Christian and Theis, Lucas and Huszar, Ferenc and Caballero, Jose and Cunningham, Andrew and Acosta, Alejandro and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Wang, Zehan and Shi, Wenzhe},
	month = sep,
	year = {2016},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	annote = {arXiv: 1609.04802},
	annote = {Comment: 19 pages, 15 figures, 2 tables, accepted for oral presentation at CVPR, main paper + some supplementary material},
	file = {arXiv\:1609.04802 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/3XGCFZPK/Ledig et al. - 2016 - Photo-Realistic Single Image Super-Resolution Usin.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/BS5R8XYN/1609.html:text/html}
}

@article{souly_semi_2017,
	title = {Semi and {Weakly} {Supervised} {Semantic} {Segmentation} {Using} {Generative} {Adversarial} {Network}},
	url = {http://arxiv.org/abs/1703.09695},
	abstract = {Semantic segmentation has been a long standing challenging task in computer vision. It aims at assigning a label to each image pixel and needs significant number of pixellevel annotated data, which is often unavailable. To address this lack, in this paper, we leverage, on one hand, massive amount of available unlabeled or weakly labeled data, and on the other hand, non-real images created through Generative Adversarial Networks. In particular, we propose a semi-supervised framework ,based on Generative Adversarial Networks (GANs), which consists of a generator network to provide extra training examples to a multi-class classifier, acting as discriminator in the GAN framework, that assigns sample a label y from the K possible classes or marks it as a fake sample (extra class). The underlying idea is that adding large fake visual data forces real samples to be close in the feature space, enabling a bottom-up clustering process, which, in turn, improves multiclass pixel classification. To ensure higher quality of generated images for GANs with consequent improved pixel classification, we extend the above framework by adding weakly annotated data, i.e., we provide class level information to the generator. We tested our approaches on several challenging benchmarking visual datasets, i.e. PASCAL, SiftFLow, Stanford and CamVid, achieving competitive performance also compared to state-of-the-art semantic segmentation method},
	urldate = {2018-03-09},
	journal = {arXiv:1703.09695 [cs]},
	author = {Souly, Nasim and Spampinato, Concetto and Shah, Mubarak},
	month = mar,
	year = {2017},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {arXiv: 1703.09695},
	file = {arXiv\:1703.09695 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/BERFF362/Souly et al. - 2017 - Semi and Weakly Supervised Semantic Segmentation U.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/QDCFF7HS/1703.html:text/html}
}

@article{xue_segan:_2017,
	title = {{SegAN}: {Adversarial} {Network} with {Multi}-scale \${L}\_1\$ {Loss} for {Medical} {Image} {Segmentation}},
	shorttitle = {{SegAN}},
	url = {http://arxiv.org/abs/1706.01805},
	abstract = {Inspired by classic generative adversarial networks (GAN), we propose a novel end-to-end adversarial neural network, called SegAN, for the task of medical image segmentation. Since image segmentation requires dense, pixel-level labeling, the single scalar real/fake output of a classic GAN's discriminator may be ineffective in producing stable and sufficient gradient feedback to the networks. Instead, we use a fully convolutional neural network as the segmentor to generate segmentation label maps, and propose a novel adversarial critic network with a multi-scale \$L\_1\$ loss function to force the critic and segmentor to learn both global and local features that capture long- and short-range spatial relationships between pixels. In our SegAN framework, the segmentor and critic networks are trained in an alternating fashion in a min-max game: The critic takes as input a pair of images, (original\_image \$*\$ predicted\_label\_map, original\_image \$*\$ ground\_truth\_label\_map), and then is trained by maximizing a multi-scale loss function; The segmentor is trained with only gradients passed along by the critic, with the aim to minimize the multi-scale loss function. We show that such a SegAN framework is more effective and stable for the segmentation task, and it leads to better performance than the state-of-the-art U-net segmentation method. We tested our SegAN method using datasets from the MICCAI BRATS brain tumor segmentation challenge. Extensive experimental results demonstrate the effectiveness of the proposed SegAN with multi-scale loss: on BRATS 2013 SegAN gives performance comparable to the state-of-the-art for whole tumor and tumor core segmentation while achieves better precision and sensitivity for Gd-enhance tumor core segmentation; on BRATS 2015 SegAN achieves better performance than the state-of-the-art in both dice score and precision.},
	urldate = {2018-03-09},
	journal = {arXiv:1706.01805 [cs]},
	author = {Xue, Yuan and Xu, Tao and Zhang, Han and Long, Rodney and Huang, Xiaolei},
	month = jun,
	year = {2017},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {arXiv: 1706.01805},
	file = {arXiv\:1706.01805 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/47IVIDHX/Xue et al. - 2017 - SegAN Adversarial Network with Multi-scale \$L_1\$ .pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/GRRD9GUK/1706.html:text/html}
}

@article{berni_thermal_2009,
	title = {Thermal and {Narrowband} {Multispectral} {Remote} {Sensing} for {Vegetation} {Monitoring} {From} an {Unmanned} {Aerial} {Vehicle}},
	volume = {47},
	issn = {0196-2892},
	doi = {10.1109/TGRS.2008.2010457},
	abstract = {Two critical limitations for using current satellite sensors in real-time crop management are the lack of imagery with optimum spatial and spectral resolutions and an unfavorable revisit time for most crop stress-detection applications. Alternatives based on manned airborne platforms are lacking due to their high operational costs. A fundamental requirement for providing useful remote sensing products in agriculture is the capacity to combine high spatial resolution and quick turnaround times. Remote sensing sensors placed on unmanned aerial vehicles (UAVs) could fill this gap, providing low-cost approaches to meet the critical requirements of spatial, spectral, and temporal resolutions. This paper demonstrates the ability to generate quantitative remote sensing products by means of a helicopter-based UAV equipped with inexpensive thermal and narrowband multispectral imaging sensors. During summer of 2007, the platform was flown over agricultural fields, obtaining thermal imagery in the 7.5-13-mum region (40-cm resolution) and narrowband multispectral imagery in the 400-800-nm spectral region (20-cm resolution). Surface reflectance and temperature imagery were obtained, after atmospheric corrections with MODTRAN. Biophysical parameters were estimated using vegetation indices, namely, normalized difference vegetation index, transformed chlorophyll absorption in reflectance index/optimized soil-adjusted vegetation index, and photochemical reflectance index (PRI), coupled with SAILH and FLIGHT models. As a result, the image products of leaf area index, chlorophyll content (C ab), and water stress detection from PRI index and canopy temperature were produced and successfully validated. This paper demonstrates that results obtained with a low-cost UAV system for agricultural applications yielded comparable estimations, if not better, than those obtained by traditional manned airborne sensors.},
	number = {3},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Berni, J. A. J. and Zarco-Tejada, P. J. and Suarez, L. and Fereres, E.},
	month = mar,
	year = {2009},
	keywords = {remote sensing, chlorophyll content, crop management, crops, FLIGHT model, infrared imaging, leaf area index, MODTRAN, Multispectral, multispectral remote sensing, narrowband, photochemical reflectance index, radiative transfer modeling, reflectivity, remotely operated vehicles, SAILH model, stress detection, surface reflectance imagery, surface temperature imagery, thermal, unmanned aerial system (UAS), unmanned aerial vehicle, unmanned aerial vehicles (UAVs), vegetation mapping, vegetation monitoring, water stress detection, wavelength 400 nm to 800 nm, wavelength 7.5 mum to 13 mum},
	pages = {722--738},
	file = {IEEE Xplore Abstract Record:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/5VYNM6K5/4781575.html:text/html}
}

@article{uto_characterization_2013,
	title = {Characterization of {Rice} {Paddies} by a {UAV}-{Mounted} {Miniature} {Hyperspectral} {Sensor} {System}},
	volume = {6},
	issn = {1939-1404},
	doi = {10.1109/JSTARS.2013.2250921},
	abstract = {A low-cost, small, lightweight hyperspectral sensor system that can be loaded onto small unmanned autonomous vehicle (UAV) platforms has been developed for the acquisition of aerial hyperspectral data. Safe and easy observation is possible under unstable illumination conditions by using lightweight and autonomous cruising. The hyperspectral sensor system, equipped with a 256-band hyperspectral sensor covering a spectral range from 340-763 nm, a GPS and a data logger, is 400 g in total weight. The acquisition period for each sampling, 768 bytes, is 100 ms. The aerial hyperspectral data of rice paddies are collected under cloudy weather. The flight altitude from the ground is 10 m, and the cruising speed is 2 m/s. The high-accuracy estimation of the chlorophyll densities is confirmed, even under unstable illumination conditions, by frequent monitoring of the illumination level and the chlorophyll indices, based on the red-edge (RE) and near infrared (NIR) spectral ranges.},
	number = {2},
	journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
	author = {Uto, K. and Seki, H. and Saito, G. and Kosugi, Y.},
	month = apr,
	year = {2013},
	keywords = {image classification, aerial hyperspectral data acquisition, agriculture, autonomous aerial vehicles, chlorophyll, chlorophyll indices, cloudy weather, Data acquisition, flight altitude, geophysical image processing, Global Positioning System, hyperspectral image, Hyperspectral imaging, illumination condition, near infrared spectral range, red edge spectral range, rice paddies classification, Sensor systems, terrain mapping, time 100 ms, UAV, UAV mounted miniature hyperspectral sensor system, unmanned autonomous vehicle, Vegetation mapping, wavelength 340 nm to 763 nm},
	pages = {851--860},
	file = {IEEE Xplore Abstract Record:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/2KCCA9XZ/6482672.html:text/html}
}

@article{moranduzzo_automatic_2014,
	title = {Automatic {Car} {Counting} {Method} for {Unmanned} {Aerial} {Vehicle} {Images}},
	volume = {52},
	issn = {0196-2892},
	doi = {10.1109/TGRS.2013.2253108},
	abstract = {This paper presents a solution to solve the car detection and counting problem in images acquired by means of unmanned aerial vehicles (UAVs). UAV images are characterized by a very high spatial resolution (order of few centimeters), and consequently by an extremely high level of details which calls for appropriate automatic analysis methods. The proposed method starts with a screening step of asphalted zones in order to restrict the areas where to detect cars and thus to reduce false alarms. Then, it performs a feature extraction process based on scalar invariant feature transform thanks to which a set of keypoints is identified in the considered image and opportunely described. Successively, it discriminates between keypoints assigned to cars and all the others, by means of a support vector machine classifier. The last step of our method is focused on the grouping of the keypoints belonging to the same car in order to get a “one keypoint-one car” relationship. Finally, the number of cars present in the scene is given by the number of final keypoints identified. The experimental results obtained on a real UAV scene characterized by a spatial resolution of 2 cm show that the proposed method exhibits a promising car counting accuracy.},
	number = {3},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Moranduzzo, T. and Melgani, F.},
	month = mar,
	year = {2014},
	keywords = {remote sensing, autonomous aerial vehicles, appropriate automatic analysis methods, asphalted zone screening step, automatic car counting method, automobiles, car counting accuracy, car counting problem, Car detection, car detection problem, car number, extraction process, extremely high detail level, feature extraction, final keypoint number, image recognition, keypoint grouping, keypoint set, one keypoint-one car relationship, real UAV scene, reduce false alarms, scalar invariant feature transform, scale invariant feature transform (SIFT), spatial resolution, support vector machine (SVM), support vector machine classifier, support vector machines, traffic engineering computing, UAV images, unmanned aerial vehicle (UAV), unmanned aerial vehicle images, very high spatial resolution},
	pages = {1635--1647},
	file = {IEEE Xplore Abstract Record:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/I67NTLJZ/6514618.html:text/html}
}

@inproceedings{ruhe_traffic_2003,
	title = {Traffic monitoring and traffic flow measurement by remote sensing systems},
	volume = {1},
	doi = {10.1109/ITSC.2003.1252053},
	abstract = {Prediction of traffic, dynamical routing, off board navigation and a standardisation of traffic flow parameters are the cornerstones of modern intelligent transport systems. Development of such systems requires intelligent data acquisition from different sensors and platforms.},
	booktitle = {Proceedings of the 2003 {IEEE} {International} {Conference} on {Intelligent} {Transportation} {Systems}},
	author = {Ruhe, M. H. O. and Dalaff, C. and Kuhne, R. D.},
	month = oct,
	year = {2003},
	keywords = {remote sensing, Data acquisition, Communication system traffic control, data acquisition, dynamical routing, Fluid flow measurement, intelligent data acquisition, Intelligent sensors, Intelligent systems, intelligent transport systems, navigation, Navigation, off board navigation, Remote monitoring, remote sensing systems, Roads, sensors, Space technology, standardisation, Telecommunication traffic, traffic control, traffic flow measurement, traffic flow parameter standardisation, traffic monitoring, traffic prediction},
	pages = {760--764 vol.1},
	file = {IEEE Xplore Abstract Record:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/NC2N4PX6/1252053.html:text/html}
}

@inproceedings{moranduzzo_lbp-based_2015,
	title = {{LBP}-based multiclass classification method for {UAV} imagery},
	doi = {10.1109/IGARSS.2015.7326283},
	abstract = {In order to describe images acquired with unmanned aerial vehicles (UAV), we introduce in this paper a multilabeling classification method. It starts by subdividing the original UAV image into a grid of tiles which are then analyzed separately. From each tile, a signature which encodes texture information is extracted and compared with the signatures of the tiles belonging to a pre-built training dictionary in order to acquire the binary multilabel vector of the most similar tile. In order to represent and match the tiles, we exploit a well-known texture operator and a common distance measure, respectively. Promising experimental results, in particular for some classes of objects, are obtained on real UAV images acquired over urban areas.},
	booktitle = {2015 {IEEE} {International} {Geoscience} and {Remote} {Sensing} {Symposium} ({IGARSS})},
	author = {Moranduzzo, T. and Mekhalfi, M. L. and Melgani, F.},
	month = jul,
	year = {2015},
	keywords = {UAV imagery, Image analysis, image classification, image texture, remote sensing, Satellites, unmanned aerial vehicle, autonomous aerial vehicles, geophysical image processing, binary multilabel vector, Dictionaries, Histograms, Image resolution, LBP-based multiclass classification method, local binary pattern, local binary pattern (LBP), multilabeling classification, multilabeling classification method, Sensitivity, similarity measures, texture information, texture operator, Training, Unmanned aerial vehicles, unmanned aerial vehicles (UAV), urban area},
	pages = {2362--2365},
	file = {IEEE Xplore Abstract Record:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/EH84NKUB/7326283.html:text/html}
}

@article{polzounov_right_2016,
	title = {Right whale recognition using convolutional neural networks},
	url = {http://arxiv.org/abs/1604.05605},
	abstract = {We studied the feasibility of recognizing individual right whales (Eubalaena glacialis) using convolutional neural networks. Prior studies have shown that CNNs can be used in wide range of classification and categorization tasks such as automated human face recognition. To test applicability of deep learning to whale recognition we have developed several models based on best practices from literature. Here, we describe the performance of the models. We conclude that machine recognition of whales is feasible and comment on the difficulty of the problem},
	urldate = {2018-03-09},
	journal = {arXiv:1604.05605 [cs]},
	author = {Polzounov, Andrei and Terpugova, Ilmira and Skiparis, Deividas and Mihai, Andrei},
	month = apr,
	year = {2016},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, 68},
	annote = {arXiv: 1604.05605},
	annote = {Comment: 10 pages + appendix, 15 figures},
	file = {arXiv\:1604.05605 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/GXR5TISG/Polzounov et al. - 2016 - Right whale recognition using convolutional neural.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/4UJMKV3J/1604.html:text/html}
}

@article{he_mask_2017,
	title = {Mask {R}-{CNN}},
	url = {http://arxiv.org/abs/1703.06870},
	abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron},
	urldate = {2018-03-09},
	journal = {arXiv:1703.06870 [cs]},
	author = {He, Kaiming and Gkioxari, Georgia and Dollár, Piotr and Girshick, Ross},
	month = mar,
	year = {2017},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {arXiv: 1703.06870},
	annote = {Comment: open source; appendix on more results},
	file = {arXiv\:1703.06870 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/YZP8LZWW/He et al. - 2017 - Mask R-CNN.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/U2BPBWAM/1703.html:text/html}
}

@inproceedings{sakla_deep_2017,
	title = {Deep {Multi}-modal {Vehicle} {Detection} in {Aerial} {ISR} {Imagery}},
	doi = {10.1109/WACV.2017.107},
	abstract = {Since the introduction of deep convolutional neural networks (CNNs), object detection in imagery has witnessed substantial breakthroughs in state-of-the-art performance. The defense community utilizes overhead image sensors that acquire large field-of-view aerial imagery in various bands of the electromagnetic spectrum, which is then exploited for various applications, including the detection and localization of man-made objects. In this work, we utilize a recent state-of-the art object detection algorithm, faster R-CNN, to train a deep CNN for vehicle detection in multimodal imagery. We utilize the vehicle detection in aerial imagery (VEDAI) dataset, which contains overhead imagery that is representative of an ISR setting. Our contribution includes modification of key parameters in the faster R-CNN algorithm for this setting where the objects of interest are spatially small, occupying less than 1:5×10-3 of the total image pixels. Our experiments show that (1) an appropriately trained deep CNN leads to average precision rates above 93\% on vehicle detection, and (2) transfer learning between imagery modalities is possible, yielding average precision rates above 90\% in the absence of fine-tuning.},
	booktitle = {2017 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	author = {Sakla, W. and Konjevod, G. and Mundhenk, T. N.},
	month = mar,
	year = {2017},
	keywords = {Spatial resolution, geophysical image processing, traffic engineering computing, aerial ISR Imagery, CNN, Computer vision, deep convolutional neural networks, deep multimodal vehicle detection, defense community, electromagnetic spectrum, Image color analysis, image pixels, image sensors, neural nets, object detection, Object detection, object detection algorithm, Proposals, R-CNN algorithm, road vehicles, VEDAI dataset, Vehicle detection, vehicle detection in aerial imagery},
	pages = {916--923},
	file = {IEEE Xplore Abstract Record:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/5A9ZYY8X/7926690.html:text/html}
}

@article{audebert_usability_2016,
	title = {On the usability of deep networks for object-based image analysis},
	url = {http://arxiv.org/abs/1609.06845},
	abstract = {As computer vision before, remote sensing has been radically changed by the introduction of Convolution Neural Networks. Land cover use, object detection and scene understanding in aerial images rely more and more on deep learning to achieve new state-of-the-art results. Recent architectures such as Fully Convolutional Networks (Long et al., 2015) can even produce pixel level annotations for semantic mapping. In this work, we show how to use such deep networks to detect, segment and classify different varieties of wheeled vehicles in aerial images from the ISPRS Potsdam dataset. This allows us to tackle object detection and classification on a complex dataset made up of visually similar classes, and to demonstrate the relevance of such a subclass modeling approach. Especially, we want to show that deep learning is also suitable for object-oriented analysis of Earth Observation data. First, we train a FCN variant on the ISPRS Potsdam dataset and show how the learnt semantic maps can be used to extract precise segmentation of vehicles, which allow us studying the repartition of vehicles in the city. Second, we train a CNN to perform vehicle classification on the VEDAI (Razakarivony and Jurie, 2016) dataset, and transfer its knowledge to classify candidate segmented vehicles on the Potsdam dataset.},
	urldate = {2018-03-09},
	journal = {arXiv:1609.06845 [cs]},
	author = {Audebert, Nicolas and Saux, Bertrand Le and Lefèvre, Sébastien},
	month = sep,
	year = {2016},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	annote = {arXiv: 1609.06845},
	annote = {Comment: in International Conference on Geographic Object-Based Image Analysis (GEOBIA), Sep 2016, Enschede, Netherlands},
	file = {arXiv\:1609.06845 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/5FW3UZNT/Audebert et al. - 2016 - On the usability of deep networks for object-based.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/HEYKVS9F/1609.html:text/html}
}

@book{stockman_computer_2001,
	address = {Upper Saddle River, NJ, USA},
	edition = {1st},
	title = {Computer {Vision}},
	isbn = {0-13-030796-3},
	publisher = {Prentice Hall PTR},
	author = {Stockman, George and Shapiro, Linda G.},
	year = {2001}
}

@misc{chintala_ganhacks:_2018,
	title = {ganhacks: starter from "{How} to {Train} a {GAN}?" at {NIPS}2016},
	shorttitle = {ganhacks},
	url = {https://github.com/soumith/ganhacks},
	urldate = {2018-04-04},
	author = {Chintala, Soumith},
	month = apr,
	year = {2018},
	note = {original-date: 2016-12-09T16:09:27Z},
	file = {Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/4WNT8FYK/ganhacks.html:text/html}
}

@misc{noauthor_worldview-3_nodate,
	title = {{WorldView}-3 {Satellite} {Sensor} {\textbar} {Satellite} {Imaging} {Corp}},
	url = {https://www.satimagingcorp.com/satellite-sensors/worldview-3/},
	urldate = {2018-04-16},
	file = {WorldView-3 Satellite Sensor | Satellite Imaging Corp:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/NHAG6ISI/worldview-3.html:text/html}
}

@article{zeiler_visualizing_2013,
	title = {Visualizing and {Understanding} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1311.2901},
	urldate = {2018-04-17},
	journal = {arXiv:1311.2901 [cs]},
	author = {Zeiler, Matthew D. and Fergus, Rob},
	month = nov,
	year = {2013},
	note = {arXiv: 1311.2901},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1311.2901 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/P8C55DEJ/Zeiler and Fergus - 2013 - Visualizing and Understanding Convolutional Networ.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/R857YLPH/1311.html:text/html}
}

@inproceedings{zeiler_visualizing_2014,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Visualizing and {Understanding} {Convolutional} {Networks}},
	isbn = {978-3-319-10589-5 978-3-319-10590-1},
	url = {https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53},
	doi = {10.1007/978-3-319-10590-1_53},
	abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
	language = {en},
	urldate = {2018-04-19},
	booktitle = {Computer {Vision} – {ECCV} 2014},
	publisher = {Springer, Cham},
	author = {Zeiler, Matthew D. and Fergus, Rob},
	month = sep,
	year = {2014},
	pages = {818--833},
	file = {Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/M5AGS4CQ/978-3-319-10590-1_53.html:text/html}
}

@article{zhong_robust_2017-1,
	title = {Robust {Vehicle} {Detection} in {Aerial} {Images} {Based} on {Cascaded} {Convolutional} {Neural} {Networks}},
	volume = {17},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {http://www.mdpi.com/1424-8220/17/12/2720},
	doi = {10.3390/s17122720},
	abstract = {Vehicle detection in aerial images is an important and challenging task. Traditionally, many target detection models based on sliding-window fashion were developed and achieved acceptable performance, but these models are time-consuming in the detection phase. Recently, with the great success of convolutional neural networks (CNNs) in computer vision, many state-of-the-art detectors have been designed based on deep CNNs. However, these CNN-based detectors are inefficient when applied in aerial image data due to the fact that the existing CNN-based models struggle with small-size object detection and precise localization. To improve the detection accuracy without decreasing speed, we propose a CNN-based detection model combining two independent convolutional neural networks, where the first network is applied to generate a set of vehicle-like regions from multi-feature maps of different hierarchies and scales. Because the multi-feature maps combine the advantage of the deep and shallow convolutional layer, the first network performs well on locating the small targets in aerial image data. Then, the generated candidate regions are fed into the second network for feature extraction and decision making. Comprehensive experiments are conducted on the Vehicle Detection in Aerial Imagery (VEDAI) dataset and Munich vehicle dataset. The proposed cascaded detection model yields high performance, not only in detection accuracy but also in detection speed.},
	language = {en},
	number = {12},
	urldate = {2018-04-19},
	journal = {Sensors},
	author = {Zhong, Jiandan and Lei, Tao and Yao, Guangle},
	month = nov,
	year = {2017},
	keywords = {deep learning, vehicle detection, aerial image, convolutional neural network},
	pages = {2720},
	file = {Full Text PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/85CN6X9C/Zhong et al. - 2017 - Robust Vehicle Detection in Aerial Images Based on.pdf:application/pdf;Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/L2C4SC57/2720.html:text/html}
}